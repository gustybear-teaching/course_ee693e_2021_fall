---
title: "02: Predictive Adaptive Streaming to Enable Mobile
360-degree and VR Experiences"
date: 2021-10-18
type: journal
commentable: true
# Provide the name of the presenter
summary: "Presenter(s): Alemayehu Beemnet, Banh Nguyen, Tahmid Tasmia"
# Provide other tags that describe the paper
tags:
- teaching
- ee693e
---
***
## Paper Summary

***
## Presentation
{{< youtube ynD_kXCivFE >}}
***
## Review
### Strengths


### Weaknesses


### Detailed Comments
360° videos and cloud/edge-based virtual reality applications require extremely high bandwidth and low latency. The method of predictive adaptive streaming is based on a model of viewpoint prediction developed using deep learning. It predicts the user's gaze in a 360-degree view based on previous head movements. They examine how to develop mobile (wireless and lightweight) head-mounted displays (HMDs) and how to enable VR experiences on mobile HMDs while operating on bandwidth-constrained mobile networks while meeting ultra-low latency requirements in this article. Our approach overcomes both bandwidth and latency constraints by predicting head motion.

They present a real-time predictive long-term memory (LSTM) model for 360-degree video streaming in this article. Additionally, they demonstrate significant quality, bandwidth, and network capacity benefits over current state-of-the-art streaming without bitrate adaptation (BTVR). We investigate the problem of sequence prediction in this article, which is defined as predicting the next value(s) given a historical sequence. At this stage in their respective fields, conventional machine learning and deep learning approaches to sequence prediction are not considered very effective. They are the first to suggest a method for 360-degree video streaming prediction.


Our method, which relies solely on head motion data, is more efficient and concise in addressing the ultra-low latency challenge in our current video-streaming scenarios. Because most HMDs are incapable of tracking gaze, the gaze-prediction technique cannot be implemented directly. The edge device can be a Mobile Edge Computing (MEC) node embedded within the mobile radio access or core network, or it can be a. Local Edge Computing (LEC) nodes can be located on the user's premises or even on his or her mobile device. Utilization of cloud servers for predictive view generation is also a possibility.



In each time point, a viewpoint can occur in up to K unique tiles (e.g., every 200ms). The entire predictive adaptive streaming method is decomposed into two subtasks: viewpoint prediction and adaptive streaming. Over 80% of videos have a duration of a few seconds and approximately 85% have a viewing time of several thousand hours. Head pose data for 19 online virtual reality videos are included in the dataset.



The attention map is defined as a series of probabilities that a viewpoint is contained within a tile for n viewers over the time period cts1 to cts2. Figure 6 illustrates an attention map, illustrating how users' attention was distributed during a one-second period within the Kong VR video's high-motion sequence. For viewpoint feature representation, we employ a tile-based format. Each grid is 30°30° in size, which divides the 360-degree view into 72 tiles. We chose 2s as the prediction window because it outperforms 3s, 4s, and 5s in terms of performance.



Given the previous sequence of viewpoint tiles, theirLSTM model predicts which tile will contain the viewpoint. The proposed model optimizes parameters by minimizing cross-entropy, and we train it using 30 mini-batches and 128 LSTM units. Predictive FOV streaming can help meet the ultra-low latency requirement of an immersive 360-degree or VR experience. However, wireless network bandwidth may not always be sufficient to transmit with high accuracy the predicted FOV (that is, considering a high enough number of "m" tiles). They examine how viewpoint prediction can be used to develop an adaptive streaming technique for 360-degree and virtual reality videos. They can find an optimal solution that maximizes user experience (minimizes impairmentI) given a bandwidth constraint BW(t) for the time slot t.



Next They present the experimental results and experimental setup for our proposed LSTM model. Our approach is implemented in Python using Keras on an Intel Core i7 Quad-Core processor with 32GB RAM. Up to one hour of training is required for state-of-the-art methods. As the number of tiles m increases, the accuracy of the FOV prediction increases while the number of pixels saved decreases.



Their LSTM model can predict the FOV with an accuracy of approximately 95% when only four tiles (i.e., m = 4) are used, resulting in FOV and pixel savings of approximately 70% for Fashion Show and Roller Coaster, and 66.8 percent for Whale Encounter. They use an adaptive streaming technique known as viewport-driven rate distortion optimized streaming. This method enables adaptive video streaming based on heatmaps that quantify the probability of navigating various spatial segments of a 360-degree video over time. We calculate the P(QHigh), P(QMedium), P(QLow), and PSNR in actual user FOV under various bandwidth conditions using 1000 head motion traces from a video sequence called Fashion Show.



Their predictive adaptive streaming algorithm VR-PAS achieves an average P(QHigh) of approximately 98 percent while using 25Mbps, whereas a typical HDTV stream requires 57.6Mbps in total bandwidth. When compared to conventional streaming, predictive adaptive streaming (VR-PAS) can save up to 53.3 percent on network bandwidth (i.e., non-adaptive streaming). The method achieves a PSNR of 50dB when operating within a range of fixed bandwidth limits.



As illustrated in Table IX, the predictive adaptive streaming algorithm VR-PAS can run in real time with a runtime of less than 70 milliseconds. With a round-trip transmission latency of less than 25ms and the additional delays associated with viewpoint prediction and decoding, their algorithm can be executed in real time approximately every 100ms. We present a multilayer LSTM model that is capable of learning general head motion patterns and forecasting future viewpoints. Their method outperforms state-of-the-art methods on a real-world dataset of head motion traces and demonstrates significant potential for bandwidth reduction while maintaining a positive user experience (i.e., high PSNR).

### Questions
(1) What is the normal range of bandwidth needed for gaming?
(2) What is the reason why LSTM is used for this research?
(3) What do you think is the next step for FOV probability?
